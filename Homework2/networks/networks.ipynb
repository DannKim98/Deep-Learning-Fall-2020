{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-GHYLqEfQ5t5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i6dPqtddRnVT"
   },
   "outputs": [],
   "source": [
    "train_dir = '../data/train'\n",
    "test_dir = '../data/test_local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z-5_DTD8fLQz"
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.Resize(32),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomVerticalFlip(),\n",
    "                                      transforms.RandomRotation(30),\n",
    "                                      transforms.RandomRotation(60),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "train_data, val_data = torch.utils.data.random_split(data, [4000, 537])\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=True)\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(32),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                          [0.229, 0.224, 0.225])])\n",
    "test_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yVGyGvuBupDZ"
   },
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 2, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 2, 1)\n",
    "        self.fc1 = nn.Linear(128*3*3, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define forward propagation here\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 128*3*3)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 2, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 2, 1)\n",
    "        self.fc1 = nn.Linear(128*3*3, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define forward propagation here\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 128*3*3)\n",
    "        x = self.fc1(x)\n",
    "        x - F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x - F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define forward propagation here\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 128*4*4)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net4, self).__init__()\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, 1)\n",
    "        self.fc1 = nn.Linear(64*5*5, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define forward propagation here\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 64*5*5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(64*6*6, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define forward propagation here\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 64*6*6)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net6, self).__init__()\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, 1)\n",
    "        self.fc1 = nn.Linear(64*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define forward propagation here\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 64*5*5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net7, self).__init__()\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, 1)\n",
    "        self.fc1 = nn.Linear(64*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define forward propagation here\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.avg_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.avg_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 64*5*5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net8, self).__init__()\n",
    "        # Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128*4*4, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define forward propagation here\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 128*4*4)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8XeRR3QdGuBk"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xQCMW88qBNC9"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SstIOrmFG0fq",
    "outputId": "8067df8f-defb-43a7-cf1e-eae0cbea9b1b"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 2.379796\n",
      "Train Epoch: 1 [640/4000 (16%)]\tLoss: 1.383654\n",
      "Train Epoch: 1 [1280/4000 (32%)]\tLoss: 1.656923\n",
      "Train Epoch: 1 [1920/4000 (48%)]\tLoss: 0.892638\n",
      "Train Epoch: 1 [2560/4000 (63%)]\tLoss: 0.818411\n",
      "Train Epoch: 1 [3200/4000 (79%)]\tLoss: 0.960186\n",
      "Train Epoch: 1 [3840/4000 (95%)]\tLoss: 0.979888\n",
      "\n",
      "Test set: Average loss: 0.7978, Accuracy: 365/537 (68%)\n",
      "\n",
      "Train Epoch: 2 [0/4000 (0%)]\tLoss: 0.886412\n",
      "Train Epoch: 2 [640/4000 (16%)]\tLoss: 1.089277\n",
      "Train Epoch: 2 [1280/4000 (32%)]\tLoss: 0.693872\n",
      "Train Epoch: 2 [1920/4000 (48%)]\tLoss: 0.782510\n",
      "Train Epoch: 2 [2560/4000 (63%)]\tLoss: 0.940951\n",
      "Train Epoch: 2 [3200/4000 (79%)]\tLoss: 0.972009\n",
      "Train Epoch: 2 [3840/4000 (95%)]\tLoss: 0.855019\n",
      "\n",
      "Test set: Average loss: 0.7120, Accuracy: 389/537 (72%)\n",
      "\n",
      "Train Epoch: 3 [0/4000 (0%)]\tLoss: 0.554281\n",
      "Train Epoch: 3 [640/4000 (16%)]\tLoss: 0.709680\n",
      "Train Epoch: 3 [1280/4000 (32%)]\tLoss: 0.603514\n",
      "Train Epoch: 3 [1920/4000 (48%)]\tLoss: 0.785927\n",
      "Train Epoch: 3 [2560/4000 (63%)]\tLoss: 0.635272\n",
      "Train Epoch: 3 [3200/4000 (79%)]\tLoss: 0.859220\n",
      "Train Epoch: 3 [3840/4000 (95%)]\tLoss: 0.611063\n",
      "\n",
      "Test set: Average loss: 0.6968, Accuracy: 381/537 (71%)\n",
      "\n",
      "Train Epoch: 4 [0/4000 (0%)]\tLoss: 0.505264\n",
      "Train Epoch: 4 [640/4000 (16%)]\tLoss: 0.615721\n",
      "Train Epoch: 4 [1280/4000 (32%)]\tLoss: 0.583844\n",
      "Train Epoch: 4 [1920/4000 (48%)]\tLoss: 0.604952\n",
      "Train Epoch: 4 [2560/4000 (63%)]\tLoss: 0.629387\n",
      "Train Epoch: 4 [3200/4000 (79%)]\tLoss: 0.761228\n",
      "Train Epoch: 4 [3840/4000 (95%)]\tLoss: 0.698961\n",
      "\n",
      "Test set: Average loss: 0.7052, Accuracy: 390/537 (73%)\n",
      "\n",
      "Train Epoch: 5 [0/4000 (0%)]\tLoss: 0.747652\n",
      "Train Epoch: 5 [640/4000 (16%)]\tLoss: 0.848830\n",
      "Train Epoch: 5 [1280/4000 (32%)]\tLoss: 0.604673\n",
      "Train Epoch: 5 [1920/4000 (48%)]\tLoss: 0.636802\n",
      "Train Epoch: 5 [2560/4000 (63%)]\tLoss: 0.709979\n",
      "Train Epoch: 5 [3200/4000 (79%)]\tLoss: 0.738190\n",
      "Train Epoch: 5 [3840/4000 (95%)]\tLoss: 0.672551\n",
      "\n",
      "Test set: Average loss: 0.6918, Accuracy: 392/537 (73%)\n",
      "\n",
      "Train Epoch: 6 [0/4000 (0%)]\tLoss: 0.580350\n",
      "Train Epoch: 6 [640/4000 (16%)]\tLoss: 0.718160\n",
      "Train Epoch: 6 [1280/4000 (32%)]\tLoss: 0.677022\n",
      "Train Epoch: 6 [1920/4000 (48%)]\tLoss: 0.504041\n",
      "Train Epoch: 6 [2560/4000 (63%)]\tLoss: 0.593415\n",
      "Train Epoch: 6 [3200/4000 (79%)]\tLoss: 0.866886\n",
      "Train Epoch: 6 [3840/4000 (95%)]\tLoss: 0.515547\n",
      "\n",
      "Test set: Average loss: 0.6634, Accuracy: 411/537 (77%)\n",
      "\n",
      "Train Epoch: 7 [0/4000 (0%)]\tLoss: 0.511572\n",
      "Train Epoch: 7 [640/4000 (16%)]\tLoss: 0.660663\n",
      "Train Epoch: 7 [1280/4000 (32%)]\tLoss: 0.546984\n",
      "Train Epoch: 7 [1920/4000 (48%)]\tLoss: 0.661307\n",
      "Train Epoch: 7 [2560/4000 (63%)]\tLoss: 0.596302\n",
      "Train Epoch: 7 [3200/4000 (79%)]\tLoss: 0.594400\n",
      "Train Epoch: 7 [3840/4000 (95%)]\tLoss: 0.625608\n",
      "\n",
      "Test set: Average loss: 0.6812, Accuracy: 395/537 (74%)\n",
      "\n",
      "Train Epoch: 8 [0/4000 (0%)]\tLoss: 0.510781\n",
      "Train Epoch: 8 [640/4000 (16%)]\tLoss: 0.861633\n",
      "Train Epoch: 8 [1280/4000 (32%)]\tLoss: 0.640616\n",
      "Train Epoch: 8 [1920/4000 (48%)]\tLoss: 0.840010\n",
      "Train Epoch: 8 [2560/4000 (63%)]\tLoss: 0.700238\n",
      "Train Epoch: 8 [3200/4000 (79%)]\tLoss: 0.495251\n",
      "Train Epoch: 8 [3840/4000 (95%)]\tLoss: 0.578267\n",
      "\n",
      "Test set: Average loss: 0.6449, Accuracy: 397/537 (74%)\n",
      "\n",
      "Train Epoch: 9 [0/4000 (0%)]\tLoss: 0.588565\n",
      "Train Epoch: 9 [640/4000 (16%)]\tLoss: 0.680149\n",
      "Train Epoch: 9 [1280/4000 (32%)]\tLoss: 0.312224\n",
      "Train Epoch: 9 [1920/4000 (48%)]\tLoss: 0.424745\n",
      "Train Epoch: 9 [2560/4000 (63%)]\tLoss: 0.615449\n",
      "Train Epoch: 9 [3200/4000 (79%)]\tLoss: 0.715769\n",
      "Train Epoch: 9 [3840/4000 (95%)]\tLoss: 0.636094\n",
      "\n",
      "Test set: Average loss: 0.6363, Accuracy: 408/537 (76%)\n",
      "\n",
      "Train Epoch: 10 [0/4000 (0%)]\tLoss: 0.626395\n",
      "Train Epoch: 10 [640/4000 (16%)]\tLoss: 0.509631\n",
      "Train Epoch: 10 [1280/4000 (32%)]\tLoss: 0.626616\n",
      "Train Epoch: 10 [1920/4000 (48%)]\tLoss: 0.665074\n",
      "Train Epoch: 10 [2560/4000 (63%)]\tLoss: 0.682920\n",
      "Train Epoch: 10 [3200/4000 (79%)]\tLoss: 0.729075\n",
      "Train Epoch: 10 [3840/4000 (95%)]\tLoss: 0.609711\n",
      "\n",
      "Test set: Average loss: 0.6091, Accuracy: 418/537 (78%)\n",
      "\n",
      "Train Epoch: 11 [0/4000 (0%)]\tLoss: 0.648957\n",
      "Train Epoch: 11 [640/4000 (16%)]\tLoss: 0.568526\n",
      "Train Epoch: 11 [1280/4000 (32%)]\tLoss: 0.615288\n",
      "Train Epoch: 11 [1920/4000 (48%)]\tLoss: 0.570694\n",
      "Train Epoch: 11 [2560/4000 (63%)]\tLoss: 0.787640\n",
      "Train Epoch: 11 [3200/4000 (79%)]\tLoss: 0.580084\n",
      "Train Epoch: 11 [3840/4000 (95%)]\tLoss: 0.620485\n",
      "\n",
      "Test set: Average loss: 0.5940, Accuracy: 411/537 (77%)\n",
      "\n",
      "Train Epoch: 12 [0/4000 (0%)]\tLoss: 0.750076\n",
      "Train Epoch: 12 [640/4000 (16%)]\tLoss: 0.743085\n",
      "Train Epoch: 12 [1280/4000 (32%)]\tLoss: 0.486403\n",
      "Train Epoch: 12 [1920/4000 (48%)]\tLoss: 0.532749\n",
      "Train Epoch: 12 [2560/4000 (63%)]\tLoss: 0.566865\n",
      "Train Epoch: 12 [3200/4000 (79%)]\tLoss: 0.551797\n",
      "Train Epoch: 12 [3840/4000 (95%)]\tLoss: 0.632858\n",
      "\n",
      "Test set: Average loss: 0.6023, Accuracy: 419/537 (78%)\n",
      "\n",
      "Train Epoch: 13 [0/4000 (0%)]\tLoss: 0.634977\n",
      "Train Epoch: 13 [640/4000 (16%)]\tLoss: 0.611651\n",
      "Train Epoch: 13 [1280/4000 (32%)]\tLoss: 0.856120\n",
      "Train Epoch: 13 [1920/4000 (48%)]\tLoss: 0.446761\n",
      "Train Epoch: 13 [2560/4000 (63%)]\tLoss: 0.556213\n",
      "Train Epoch: 13 [3200/4000 (79%)]\tLoss: 0.663348\n",
      "Train Epoch: 13 [3840/4000 (95%)]\tLoss: 0.464724\n",
      "\n",
      "Test set: Average loss: 0.6105, Accuracy: 421/537 (78%)\n",
      "\n",
      "Train Epoch: 14 [0/4000 (0%)]\tLoss: 0.563828\n",
      "Train Epoch: 14 [640/4000 (16%)]\tLoss: 0.635371\n",
      "Train Epoch: 14 [1280/4000 (32%)]\tLoss: 0.515195\n",
      "Train Epoch: 14 [1920/4000 (48%)]\tLoss: 0.638435\n",
      "Train Epoch: 14 [2560/4000 (63%)]\tLoss: 0.596587\n",
      "Train Epoch: 14 [3200/4000 (79%)]\tLoss: 0.479102\n",
      "Train Epoch: 14 [3840/4000 (95%)]\tLoss: 0.379934\n",
      "\n",
      "Test set: Average loss: 0.6309, Accuracy: 405/537 (75%)\n",
      "\n",
      "Train Epoch: 15 [0/4000 (0%)]\tLoss: 0.742660\n",
      "Train Epoch: 15 [640/4000 (16%)]\tLoss: 0.603026\n",
      "Train Epoch: 15 [1280/4000 (32%)]\tLoss: 0.667694\n",
      "Train Epoch: 15 [1920/4000 (48%)]\tLoss: 0.517651\n",
      "Train Epoch: 15 [2560/4000 (63%)]\tLoss: 0.630624\n",
      "Train Epoch: 15 [3200/4000 (79%)]\tLoss: 0.675687\n",
      "Train Epoch: 15 [3840/4000 (95%)]\tLoss: 0.474242\n",
      "\n",
      "Test set: Average loss: 0.6044, Accuracy: 417/537 (78%)\n",
      "\n",
      "Train Epoch: 16 [0/4000 (0%)]\tLoss: 0.451019\n",
      "Train Epoch: 16 [640/4000 (16%)]\tLoss: 0.590467\n",
      "Train Epoch: 16 [1280/4000 (32%)]\tLoss: 0.600680\n",
      "Train Epoch: 16 [1920/4000 (48%)]\tLoss: 0.460871\n",
      "Train Epoch: 16 [2560/4000 (63%)]\tLoss: 0.689754\n",
      "Train Epoch: 16 [3200/4000 (79%)]\tLoss: 0.473548\n",
      "Train Epoch: 16 [3840/4000 (95%)]\tLoss: 0.625339\n",
      "\n",
      "Test set: Average loss: 0.6006, Accuracy: 421/537 (78%)\n",
      "\n",
      "Train Epoch: 17 [0/4000 (0%)]\tLoss: 0.492035\n",
      "Train Epoch: 17 [640/4000 (16%)]\tLoss: 0.551988\n",
      "Train Epoch: 17 [1280/4000 (32%)]\tLoss: 0.319273\n",
      "Train Epoch: 17 [1920/4000 (48%)]\tLoss: 0.458052\n",
      "Train Epoch: 17 [2560/4000 (63%)]\tLoss: 0.543827\n",
      "Train Epoch: 17 [3200/4000 (79%)]\tLoss: 0.396268\n",
      "Train Epoch: 17 [3840/4000 (95%)]\tLoss: 0.551856\n",
      "\n",
      "Test set: Average loss: 0.5785, Accuracy: 424/537 (79%)\n",
      "\n",
      "Train Epoch: 18 [0/4000 (0%)]\tLoss: 0.609124\n",
      "Train Epoch: 18 [640/4000 (16%)]\tLoss: 0.671634\n",
      "Train Epoch: 18 [1280/4000 (32%)]\tLoss: 0.515329\n",
      "Train Epoch: 18 [1920/4000 (48%)]\tLoss: 0.711520\n",
      "Train Epoch: 18 [2560/4000 (63%)]\tLoss: 0.503450\n",
      "Train Epoch: 18 [3200/4000 (79%)]\tLoss: 0.393818\n",
      "Train Epoch: 18 [3840/4000 (95%)]\tLoss: 0.487584\n",
      "\n",
      "Test set: Average loss: 0.5803, Accuracy: 421/537 (78%)\n",
      "\n",
      "Train Epoch: 19 [0/4000 (0%)]\tLoss: 0.423332\n",
      "Train Epoch: 19 [640/4000 (16%)]\tLoss: 0.434998\n",
      "Train Epoch: 19 [1280/4000 (32%)]\tLoss: 0.603525\n",
      "Train Epoch: 19 [1920/4000 (48%)]\tLoss: 0.638760\n",
      "Train Epoch: 19 [2560/4000 (63%)]\tLoss: 0.566168\n",
      "Train Epoch: 19 [3200/4000 (79%)]\tLoss: 0.525931\n",
      "Train Epoch: 19 [3840/4000 (95%)]\tLoss: 0.548348\n",
      "\n",
      "Test set: Average loss: 0.5868, Accuracy: 428/537 (80%)\n",
      "\n",
      "Train Epoch: 20 [0/4000 (0%)]\tLoss: 0.541463\n",
      "Train Epoch: 20 [640/4000 (16%)]\tLoss: 0.407952\n",
      "Train Epoch: 20 [1280/4000 (32%)]\tLoss: 0.526998\n",
      "Train Epoch: 20 [1920/4000 (48%)]\tLoss: 0.549408\n",
      "Train Epoch: 20 [2560/4000 (63%)]\tLoss: 0.479776\n",
      "Train Epoch: 20 [3200/4000 (79%)]\tLoss: 0.527695\n",
      "Train Epoch: 20 [3840/4000 (95%)]\tLoss: 0.601245\n",
      "\n",
      "Test set: Average loss: 0.5915, Accuracy: 425/537 (79%)\n",
      "\n",
      "Train Epoch: 21 [0/4000 (0%)]\tLoss: 0.574620\n",
      "Train Epoch: 21 [640/4000 (16%)]\tLoss: 0.667211\n",
      "Train Epoch: 21 [1280/4000 (32%)]\tLoss: 0.311481\n",
      "Train Epoch: 21 [1920/4000 (48%)]\tLoss: 0.589193\n",
      "Train Epoch: 21 [2560/4000 (63%)]\tLoss: 0.398749\n",
      "Train Epoch: 21 [3200/4000 (79%)]\tLoss: 0.500617\n",
      "Train Epoch: 21 [3840/4000 (95%)]\tLoss: 0.468625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5665, Accuracy: 422/537 (79%)\n",
      "\n",
      "Train Epoch: 22 [0/4000 (0%)]\tLoss: 0.644992\n",
      "Train Epoch: 22 [640/4000 (16%)]\tLoss: 0.468344\n",
      "Train Epoch: 22 [1280/4000 (32%)]\tLoss: 0.438366\n",
      "Train Epoch: 22 [1920/4000 (48%)]\tLoss: 0.554113\n",
      "Train Epoch: 22 [2560/4000 (63%)]\tLoss: 0.591677\n",
      "Train Epoch: 22 [3200/4000 (79%)]\tLoss: 0.649116\n",
      "Train Epoch: 22 [3840/4000 (95%)]\tLoss: 0.521023\n",
      "\n",
      "Test set: Average loss: 0.5770, Accuracy: 432/537 (80%)\n",
      "\n",
      "Train Epoch: 23 [0/4000 (0%)]\tLoss: 0.536351\n",
      "Train Epoch: 23 [640/4000 (16%)]\tLoss: 0.490474\n",
      "Train Epoch: 23 [1280/4000 (32%)]\tLoss: 0.417415\n",
      "Train Epoch: 23 [1920/4000 (48%)]\tLoss: 0.579979\n",
      "Train Epoch: 23 [2560/4000 (63%)]\tLoss: 0.570350\n",
      "Train Epoch: 23 [3200/4000 (79%)]\tLoss: 0.517612\n",
      "Train Epoch: 23 [3840/4000 (95%)]\tLoss: 0.449079\n",
      "\n",
      "Test set: Average loss: 0.5896, Accuracy: 422/537 (79%)\n",
      "\n",
      "Train Epoch: 24 [0/4000 (0%)]\tLoss: 0.356818\n",
      "Train Epoch: 24 [640/4000 (16%)]\tLoss: 0.565607\n",
      "Train Epoch: 24 [1280/4000 (32%)]\tLoss: 0.441436\n",
      "Train Epoch: 24 [1920/4000 (48%)]\tLoss: 0.416232\n",
      "Train Epoch: 24 [2560/4000 (63%)]\tLoss: 0.453800\n",
      "Train Epoch: 24 [3200/4000 (79%)]\tLoss: 0.404955\n",
      "Train Epoch: 24 [3840/4000 (95%)]\tLoss: 0.499457\n",
      "\n",
      "Test set: Average loss: 0.5701, Accuracy: 427/537 (80%)\n",
      "\n",
      "Train Epoch: 25 [0/4000 (0%)]\tLoss: 0.588347\n",
      "Train Epoch: 25 [640/4000 (16%)]\tLoss: 0.462401\n",
      "Train Epoch: 25 [1280/4000 (32%)]\tLoss: 0.492622\n",
      "Train Epoch: 25 [1920/4000 (48%)]\tLoss: 0.499497\n",
      "Train Epoch: 25 [2560/4000 (63%)]\tLoss: 0.394059\n",
      "Train Epoch: 25 [3200/4000 (79%)]\tLoss: 0.374603\n",
      "Train Epoch: 25 [3840/4000 (95%)]\tLoss: 0.444818\n",
      "\n",
      "Test set: Average loss: 0.5877, Accuracy: 420/537 (78%)\n",
      "\n",
      "Train Epoch: 26 [0/4000 (0%)]\tLoss: 0.333121\n",
      "Train Epoch: 26 [640/4000 (16%)]\tLoss: 0.404219\n",
      "Train Epoch: 26 [1280/4000 (32%)]\tLoss: 0.514943\n",
      "Train Epoch: 26 [1920/4000 (48%)]\tLoss: 0.597887\n",
      "Train Epoch: 26 [2560/4000 (63%)]\tLoss: 0.420948\n",
      "Train Epoch: 26 [3200/4000 (79%)]\tLoss: 0.510177\n",
      "Train Epoch: 26 [3840/4000 (95%)]\tLoss: 0.602780\n",
      "\n",
      "Test set: Average loss: 0.5468, Accuracy: 428/537 (80%)\n",
      "\n",
      "Train Epoch: 27 [0/4000 (0%)]\tLoss: 0.458189\n",
      "Train Epoch: 27 [640/4000 (16%)]\tLoss: 0.462453\n",
      "Train Epoch: 27 [1280/4000 (32%)]\tLoss: 0.435791\n",
      "Train Epoch: 27 [1920/4000 (48%)]\tLoss: 0.431958\n",
      "Train Epoch: 27 [2560/4000 (63%)]\tLoss: 0.401691\n",
      "Train Epoch: 27 [3200/4000 (79%)]\tLoss: 0.471109\n",
      "Train Epoch: 27 [3840/4000 (95%)]\tLoss: 0.423629\n",
      "\n",
      "Test set: Average loss: 0.5320, Accuracy: 434/537 (81%)\n",
      "\n",
      "Train Epoch: 28 [0/4000 (0%)]\tLoss: 0.686851\n",
      "Train Epoch: 28 [640/4000 (16%)]\tLoss: 0.566711\n",
      "Train Epoch: 28 [1280/4000 (32%)]\tLoss: 0.515926\n",
      "Train Epoch: 28 [1920/4000 (48%)]\tLoss: 0.454224\n",
      "Train Epoch: 28 [2560/4000 (63%)]\tLoss: 0.498946\n",
      "Train Epoch: 28 [3200/4000 (79%)]\tLoss: 0.549614\n",
      "Train Epoch: 28 [3840/4000 (95%)]\tLoss: 0.511680\n",
      "\n",
      "Test set: Average loss: 0.5682, Accuracy: 430/537 (80%)\n",
      "\n",
      "Train Epoch: 29 [0/4000 (0%)]\tLoss: 0.513624\n",
      "Train Epoch: 29 [640/4000 (16%)]\tLoss: 0.458333\n",
      "Train Epoch: 29 [1280/4000 (32%)]\tLoss: 0.628863\n",
      "Train Epoch: 29 [1920/4000 (48%)]\tLoss: 0.295263\n",
      "Train Epoch: 29 [2560/4000 (63%)]\tLoss: 0.348397\n",
      "Train Epoch: 29 [3200/4000 (79%)]\tLoss: 0.572959\n",
      "Train Epoch: 29 [3840/4000 (95%)]\tLoss: 0.489863\n",
      "\n",
      "Test set: Average loss: 0.5650, Accuracy: 428/537 (80%)\n",
      "\n",
      "Train Epoch: 30 [0/4000 (0%)]\tLoss: 0.521770\n",
      "Train Epoch: 30 [640/4000 (16%)]\tLoss: 0.476812\n",
      "Train Epoch: 30 [1280/4000 (32%)]\tLoss: 0.438128\n",
      "Train Epoch: 30 [1920/4000 (48%)]\tLoss: 0.601792\n",
      "Train Epoch: 30 [2560/4000 (63%)]\tLoss: 0.472275\n",
      "Train Epoch: 30 [3200/4000 (79%)]\tLoss: 0.522861\n",
      "Train Epoch: 30 [3840/4000 (95%)]\tLoss: 0.552897\n",
      "\n",
      "Test set: Average loss: 0.5610, Accuracy: 432/537 (80%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net6().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5)\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs-20):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, val_loader)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "for epoch in range(epochs-20, epochs-10):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, val_loader)    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "for epoch in range(epochs-10, epochs):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, val_loader)\n",
    "torch.save(model.state_dict(),\"hw2_v16.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rCuzqBoPHWzS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.4882, Accuracy: 232/272 (85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL Homework2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
